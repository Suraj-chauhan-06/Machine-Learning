{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OfD1Bu5wJ-4g"
      },
      "outputs": [],
      "source": [
        "#### 1. What does R-squared represent in a regression model?\n",
        "R-squared represents the proportion of the variance in the dependent variable that is predictable from the independent variables.\n",
        "\n",
        "#### 2. What are the assumptions of linear regression?\n",
        "- Linearity\n",
        "- Independence\n",
        "- Homoscedasticity\n",
        "- Normality of residuals\n",
        "- No multicollinearity\n",
        "\n",
        "#### 3. What is the difference between R-squared and Adjusted R-squared?\n",
        "R-squared increases with additional features. Adjusted R-squared adjusts for the number of predictors, penalizing unnecessary variables.\n",
        "\n",
        "#### 4. Why do we use Mean Squared Error (MSE)?\n",
        "MSE gives a measure of prediction accuracy by penalizing larger errors more heavily due to squaring.\n",
        "\n",
        "#### 5. What does an Adjusted R-squared value of 0.85 indicate?\n",
        "It indicates that 85% of the variance is explained by the model after adjusting for the number of predictors.\n",
        "\n",
        "#### 6. How do we check for normality of residuals in linear regression?\n",
        "Using a Q-Q plot or histogram of residuals.\n",
        "\n",
        "#### 7. What is multicollinearity, and how does it impact regression?\n",
        "It refers to high correlation between independent variables. It can make coefficient estimates unstable.\n",
        "\n",
        "#### 8. What is Mean Absolute Error (MAE)?\n",
        "MAE is the average of the absolute errors between predicted and actual values.\n",
        "\n",
        "#### 9. What are the benefits of using an ML pipeline?\n",
        "- Code modularity\n",
        "- Easy experimentation\n",
        "- Preprocessing integration\n",
        "- Cross-validation compatibility\n",
        "\n",
        "#### 10. Why is RMSE considered more interpretable than MSE?\n",
        "Because RMSE is in the same unit as the target variable, while MSE is squared.\n",
        "\n",
        "#### 11. What is pickling in Python, and how is it useful in ML?\n",
        "Pickling is saving a Python object into a byte stream. Useful for saving ML models for reuse.\n",
        "\n",
        "#### 12. What does a high R-squared value mean?\n",
        "It means that a large proportion of variance in the target variable is explained by the model.\n",
        "\n",
        "#### 13. What happens if linear regression assumptions are violated?\n",
        "Results may be biased, inconsistent, or inefficient.\n",
        "\n",
        "#### 14. How can we address multicollinearity in regression?\n",
        "- Remove correlated features\n",
        "- Use regularization (e.g., Ridge)\n",
        "- Apply PCA\n",
        "\n",
        "#### 15. How can feature selection improve model performance in regression analysis?\n",
        "By removing irrelevant or redundant variables, improving model interpretability and accuracy.\n",
        "\n",
        "#### 16. How is Adjusted R-squared calculated?\n",
        "Adjusted R² = 1 - [(1 - R²)(n - 1)/(n - k - 1)], where n = observations, k = predictors.\n",
        "\n",
        "#### 17. Why is MSE sensitive to outliers?\n",
        "Because it squares the errors, giving more weight to larger errors.\n",
        "\n",
        "#### 18. What is the role of homoscedasticity in linear regression?\n",
        "Ensures consistent variance of errors across all levels of the independent variables.\n",
        "\n",
        "#### 19. What is Root Mean Squared Error (RMSE)?\n",
        "Square root of the average of squared errors between predicted and actual values.\n",
        "\n",
        "#### 20. Why is pickling considered risky?\n",
        "Unpickling untrusted data can lead to code execution vulnerabilities.\n",
        "\n",
        "#### 21. What alternatives exist to pickling for saving ML models?\n",
        "- Joblib\n",
        "- ONNX\n",
        "- PMML\n",
        "- Model export APIs (e.g., TensorFlow SavedModel)\n",
        "\n",
        "#### 22. What is heteroscedasticity, and why is it a problem?\n",
        "It refers to non-constant variance of errors. Violates regression assumptions and affects inference.\n",
        "\n",
        "#### 23. How can interaction terms enhance a regression model's predictive power?\n",
        "By capturing combined effects of variables that wouldn’t be detected individually.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Practical**"
      ],
      "metadata": {
        "id": "OhcS9gWYKR7i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 1\n",
        "df = sns.load_dataset('diamonds')\n",
        "X = df[['carat', 'depth', 'table']].dropna()\n",
        "y = df['price'].loc[X.index]\n",
        "model = LinearRegression().fit(X, y)\n",
        "y_pred = model.predict(X)\n",
        "residuals = y - y_pred\n",
        "sns.histplot(residuals, kde=True)\n",
        "plt.title(\"Distribution of Residuals\")\n",
        "plt.show()\n",
        "\n",
        "# Task 2\n",
        "mse = mean_squared_error(y, y_pred)\n",
        "mae = mean_absolute_error(y, y_pred)\n",
        "rmse = np.sqrt(mse)\n",
        "print(f\"MSE: {mse}, MAE: {mae}, RMSE: {rmse}\")\n",
        "\n",
        "# Task 3\n",
        "plt.scatter(y_pred, residuals)\n",
        "plt.title(\"Residuals vs Fitted\")\n",
        "plt.xlabel(\"Fitted values\")\n",
        "plt.ylabel(\"Residuals\")\n",
        "plt.axhline(0, color='r', linestyle='--')\n",
        "plt.show()\n",
        "\n",
        "sns.heatmap(X.corr(), annot=True, cmap='coolwarm')\n",
        "plt.title(\"Correlation Matrix\")\n",
        "plt.show()\n",
        "\n",
        "# Task 4\n",
        "pipeline = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('model', LinearRegression())\n",
        "])\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "pipeline.fit(X_train, y_train)\n",
        "print(\"Pipeline R² Score:\", pipeline.score(X_test, y_test))\n",
        "\n",
        "# Task 5\n",
        "X = df[['carat']].dropna()\n",
        "y = df['price'].loc[X.index]\n",
        "model = LinearRegression().fit(X, y)\n",
        "print(\"Coefficient:\", model.coef_)\n",
        "print(\"Intercept:\", model.intercept_)\n",
        "print(\"R² Score:\", model.score(X, y))\n"
      ],
      "metadata": {
        "id": "Iq6nyiOtKUVm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 6\n",
        "tips = sns.load_dataset('tips')\n",
        "X = tips[['total_bill']]\n",
        "y = tips['tip']\n",
        "model = LinearRegression().fit(X, y)\n",
        "sns.regplot(x='total_bill', y='tip', data=tips)\n",
        "plt.title(\"Tip vs Total Bill\")\n",
        "plt.show()\n",
        "\n",
        "# Task 7\n",
        "X = np.random.rand(100, 1) * 10\n",
        "y = 3 * X.flatten() + np.random.randn(100) * 5\n",
        "model = LinearRegression().fit(X, y)\n",
        "plt.scatter(X, y)\n",
        "plt.plot(X, model.predict(X), color='red')\n",
        "plt.title(\"Synthetic Linear Regression\")\n",
        "plt.show()\n",
        "\n",
        "# Task 8\n",
        "with open(\"linear_model.pkl\", \"wb\") as f:\n",
        "    pickle.dump(model, f)\n",
        "\n",
        "# Task 9\n",
        "X = np.linspace(0, 10, 100)\n",
        "y = 2 * X**2 + 3 * X + 5 + np.random.randn(100) * 10\n",
        "X_poly = PolynomialFeatures(degree=2).fit_transform(X.reshape(-1, 1))\n",
        "model = LinearRegression().fit(X_poly, y)\n",
        "plt.scatter(X, y)\n",
        "plt.plot(X, model.predict(X_poly), color='red')\n",
        "plt.title(\"Polynomial Regression Curve (Degree 2)\")\n",
        "plt.show()\n",
        "\n",
        "# Task 10\n",
        "X, y = make_regression(n_samples=100, n_features=1, noise=15)\n",
        "model = LinearRegression().fit(X, y)\n",
        "print(\"Coefficient:\", model.coef_)\n",
        "print(\"Intercept:\", model.intercept_)\n"
      ],
      "metadata": {
        "id": "jWAv9TO8KWhA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 11\n",
        "for d in range(1, 5):\n",
        "    poly = PolynomialFeatures(degree=d)\n",
        "    X_poly = poly.fit_transform(X)\n",
        "    model = LinearRegression().fit(X_poly, y)\n",
        "    print(f\"Degree {d} R² Score:\", model.score(X_poly, y))\n",
        "\n",
        "# Task 12\n",
        "X, y = make_regression(n_samples=100, n_features=2, noise=10)\n",
        "model = LinearRegression().fit(X, y)\n",
        "print(\"Coefficients:\", model.coef_)\n",
        "print(\"Intercept:\", model.intercept_)\n",
        "print(\"R² Score:\", model.score(X, y))\n",
        "\n",
        "# Task 13\n",
        "plt.scatter(X[:, 0], y)\n",
        "plt.plot(X[:, 0], model.predict(X), color='red')\n",
        "plt.title(\"Regression Line with Data\")\n",
        "plt.show()\n",
        "\n",
        "# Task 14\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "df = pd.DataFrame(X, columns=['x1', 'x2'])\n",
        "vif = pd.DataFrame()\n",
        "vif[\"VIF\"] = [variance_inflation_factor(df.values, i) for i in range(df.shape[1])]\n",
        "vif[\"Feature\"] = df.columns\n",
        "print(vif)\n",
        "\n",
        "# Task 15\n",
        "X = np.linspace(-3, 3, 100).reshape(-1, 1)\n",
        "y = 1 + 2*X + 3*X**2 + 4*X**3 + 5*X**4 + np.random.randn(100, 1).flatten()\n",
        "poly = PolynomialFeatures(degree=4)\n",
        "X_poly = poly.fit_transform(X)\n",
        "model = LinearRegression().fit(X_poly, y)\n",
        "plt.scatter(X, y)\n",
        "plt.plot(X, model.predict(X_poly), color='red')\n",
        "plt.title(\"Polynomial Regression (Degree 4)\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "sdcQZZ2GKZSL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 16\n",
        "pipe = Pipeline([\n",
        "    (\"scale\", StandardScaler()),\n",
        "    (\"lr\", LinearRegression())\n",
        "])\n",
        "pipe.fit(X, y)\n",
        "print(\"R² Score:\", pipe.score(X, y))\n",
        "\n",
        "# Task 17\n",
        "poly = PolynomialFeatures(degree=3)\n",
        "X_poly = poly.fit_transform(X)\n",
        "model = LinearRegression().fit(X_poly, y)\n",
        "plt.scatter(X, y)\n",
        "plt.plot(X, model.predict(X_poly), color='red')\n",
        "plt.title(\"Polynomial Regression (Degree 3)\")\n",
        "plt.show()\n",
        "\n",
        "# Task 18\n",
        "X, y = make_regression(n_samples=100, n_features=5, noise=10)\n",
        "model = LinearRegression().fit(X, y)\n",
        "print(\"R² Score:\", model.score(X, y))\n",
        "print(\"Coefficients:\", model.coef_)\n",
        "\n",
        "# Task 19\n",
        "X, y = make_regression(n_samples=100, n_features=1, noise=10)\n",
        "model = LinearRegression().fit(X, y)\n",
        "plt.scatter(X, y)\n",
        "plt.plot(X, model.predict(X), color='red')\n",
        "plt.title(\"Regression Line\")\n",
        "plt.show()\n",
        "\n",
        "# Task 20\n",
        "X, y = make_regression(n_samples=100, n_features=3, noise=10)\n",
        "model = LinearRegression().fit(X, y)\n",
        "print(\"R² Score:\", model.score(X, y))\n",
        "print(\"Coefficients:\", model.coef_)\n"
      ],
      "metadata": {
        "id": "nMYf4v-RKbta"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 21\n",
        "joblib.dump(model, \"model_joblib.pkl\")\n",
        "model_loaded = joblib.load(\"model_joblib.pkl\")\n",
        "print(\"Deserialized Model R² Score:\", model_loaded.score(X, y))\n",
        "\n",
        "# Task 22\n",
        "tips = sns.load_dataset('tips')\n",
        "X = tips[['total_bill', 'sex', 'smoker']]\n",
        "y = tips['tip']\n",
        "X_encoded = pd.get_dummies(X, drop_first=True)\n",
        "model = LinearRegression().fit(X_encoded, y)\n",
        "print(\"R² Score:\", model.score(X_encoded, y))\n",
        "\n",
        "# Task 23\n",
        "X, y = make_regression(n_samples=100, n_features=1, noise=20)\n",
        "lr = LinearRegression().fit(X, y)\n",
        "rr = Ridge(alpha=1.0).fit(X, y)\n",
        "print(\"Linear:\", lr.coef_, lr.score(X, y))\n",
        "print(\"Ridge:\", rr.coef_, rr.score(X, y))\n",
        "\n",
        "# Task 24\n",
        "X, y = make_regression(n_samples=100, n_features=1, noise=10)\n",
        "model = LinearRegression()\n",
        "scores = cross_val_score(model, X, y, cv=5, scoring='r2')\n",
        "print(\"Cross-Validation R² Scores:\", scores)\n",
        "\n",
        "# Task 25\n",
        "for d in range(1, 6):\n",
        "    poly = PolynomialFeatures(degree=d)\n",
        "    X_poly = poly.fit_transform(X)\n",
        "    model = LinearRegression().fit(X_poly, y)\n",
        "    print(f\"Degree {d} R² Score:\", model.score(X_poly, y))\n"
      ],
      "metadata": {
        "id": "M0qKdKSjKeEo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}