{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1. What is Logistic Regression, and how does it differ from Linear Regression?\n",
        "Ans: Logistic Regression predicts probabilities for classification problems (output 0 or 1), while Linear Regression predicts continuous values.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "2. What is the mathematical equation of Logistic Regression?\n",
        "Ans:\n",
        "\n",
        "ùëÉ\n",
        "(\n",
        "ùë¶\n",
        "=\n",
        "1\n",
        "‚à£\n",
        "ùë•\n",
        ")\n",
        "=\n",
        "1\n",
        "1\n",
        "+\n",
        "ùëí\n",
        "‚àí\n",
        "(\n",
        "ùõΩ\n",
        "0\n",
        "+\n",
        "ùõΩ\n",
        "1\n",
        "ùë•\n",
        "1\n",
        "+\n",
        "‚ãØ\n",
        "+\n",
        "ùõΩ\n",
        "ùëõ\n",
        "ùë•\n",
        "ùëõ\n",
        ")\n",
        "P(y=1‚à£x)=\n",
        "1+e\n",
        "‚àí(Œ≤\n",
        "0\n",
        "‚Äã\n",
        " +Œ≤\n",
        "1\n",
        "‚Äã\n",
        " x\n",
        "1\n",
        "‚Äã\n",
        " +‚ãØ+Œ≤\n",
        "n\n",
        "‚Äã\n",
        " x\n",
        "n\n",
        "‚Äã\n",
        " )\n",
        "\n",
        "1\n",
        "‚Äã\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "3. Why do we use the Sigmoid function in Logistic Regression?\n",
        "Ans: The sigmoid function maps any real-valued number to a range between 0 and 1, making it ideal for binary classification.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "5. What is Regularization in Logistic Regression? Why is it needed?\n",
        "Ans: Regularization (L1/L2) prevents overfitting by penalizing large coefficients in the model.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "6. Explain the difference between Lasso, Ridge, and Elastic Net regression.\n",
        "Ans:\n",
        "\n",
        "Lasso (L1): Shrinks some coefficients to zero (feature selection).\n",
        "\n",
        "Ridge (L2): Shrinks all coefficients, but none to zero.\n",
        "\n",
        "Elastic Net: Combines L1 and L2.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "7. When should we use Elastic Net instead of Lasso or Ridge?\n",
        "Ans: When you have many correlated features or want a balance between feature selection and coefficient shrinkage.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "8. What is the impact of the regularization parameter (Œª or C)?\n",
        "Ans: Higher Œª (lower C) means stronger regularization, reducing overfitting but possibly underfitting.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "9. What are the key assumptions of Logistic Regression?\n",
        "Ans:\n",
        "\n",
        "Linear relationship between features and log-odds\n",
        "\n",
        "No multicollinearity\n",
        "\n",
        "Independence of observations\n",
        "\n",
        "Large sample size\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "10. What are some alternatives to Logistic Regression for classification tasks?\n",
        "Ans:\n",
        "\n",
        "Decision Trees\n",
        "\n",
        "Random Forest\n",
        "\n",
        "SVM\n",
        "\n",
        "K-NN\n",
        "\n",
        "Naive Bayes\n",
        "\n",
        "Neural Networks\n",
        "\n",
        "\n",
        "\n",
        "11. What are Classification Evaluation Metrics?\n",
        "Ans: Accuracy, Precision, Recall, F1-Score, ROC-AUC, Confusion Matrix, etc.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "12. How does class imbalance affect Logistic Regression?\n",
        "Ans: It may bias the model toward the majority class. Solutions include resampling, class weights, or synthetic data.\n",
        "\n",
        "\n",
        "13. What is Hyperparameter Tuning in Logistic Regression?\n",
        "Ans: Process of optimizing model parameters (like C, penalty) using methods like GridSearchCV or RandomizedSearchCV.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "14. What are different solvers in Logistic Regression? Which one should be used?\n",
        "Ans:\n",
        "\n",
        "liblinear: For small datasets, supports L1\n",
        "\n",
        "saga: For large datasets, supports L1/L2/elasticnet\n",
        "\n",
        "lbfgs: Fast and accurate for L2\n",
        "\n",
        "newton-cg: For L2\n",
        "\n",
        "\n",
        "\n",
        "15. How is Logistic Regression extended for multiclass classification?\n",
        "Ans: Using strategies like One-vs-Rest (OvR) or Softmax (multinomial).\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "16. What are the advantages and disadvantages of Logistic Regression?\n",
        "Ans:\n",
        "\n",
        "‚úÖ Simple, fast, interpretable\n",
        "\n",
        "‚ùå Assumes linearity, struggles with complex data\n",
        "\n",
        "\n",
        "\n",
        "17. What are some use cases of Logistic Regression?\n",
        "Ans:\n",
        "\n",
        "Spam detection\n",
        "\n",
        "Disease diagnosis\n",
        "\n",
        "Credit scoring\n",
        "\n",
        "Customer churn prediction\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "18. What is the difference between Softmax Regression and Logistic Regression?\n",
        "Ans:\n",
        "\n",
        "Logistic: Binary classification\n",
        "\n",
        "Softmax: Multiclass classification with probabilities across multiple classes\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "19. How do we choose between One-vs-Rest (OvR) and Softmax for multiclass classification?\n",
        "Ans:\n",
        "\n",
        "Use OvR for simplicity and small datasets\n",
        "\n",
        "Use Softmax for true multiclass problems with mutual exclusivity\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "20. How do we interpret coefficients in Logistic Regression?\n",
        "Ans: Each coefficient shows the change in log-odds of the target for a one-unit increase in that feature.\n",
        "\n"
      ],
      "metadata": {
        "id": "y3lxUd7xJZYT"
      },
      "id": "y3lxUd7xJZYT"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Practical"
      ],
      "metadata": {
        "id": "4NhWIgKQKsoD"
      },
      "id": "4NhWIgKQKsoD"
    },
    {
      "cell_type": "code",
      "source": [
        "#1. RandomizedSearchCV for Tuning\n",
        "\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "params = {'C': [0.01, 0.1, 1, 10], 'penalty': ['l1', 'l2'], 'solver': ['liblinear']}\n",
        "rs = RandomizedSearchCV(LogisticRegression(), params, cv=3)\n",
        "rs.fit(X_train, y_train)\n",
        "print(\"Best Params:\", rs.best_params_)\n",
        "print(\"Accuracy:\", rs.score(X_test, y_test))\n"
      ],
      "metadata": {
        "id": "CwMvoUTEKz4Y"
      },
      "id": "CwMvoUTEKz4Y",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#2. One-vs-One (OvO) Multiclass Logistic Regression\n",
        "\n",
        "from sklearn.multiclass import OneVsOneClassifier\n",
        "ovo = OneVsOneClassifier(LogisticRegression())\n",
        "ovo.fit(X_train, y_train)\n",
        "print(\"OvO Accuracy:\", ovo.score(X_test, y_test))\n"
      ],
      "metadata": {
        "id": "PZ6muxWnK3uS"
      },
      "id": "PZ6muxWnK3uS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#3. Confusion Matrix Visualization\n",
        "\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "ConfusionMatrixDisplay(cm).plot()\n"
      ],
      "metadata": {
        "id": "_j3M-troK-L5"
      },
      "id": "_j3M-troK-L5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#4. Precision, Recall, F1-Score\n",
        "\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "print(\"Precision:\", precision_score(y_test, y_pred, average='macro'))\n",
        "print(\"Recall:\", recall_score(y_test, y_pred, average='macro'))\n",
        "print(\"F1 Score:\", f1_score(y_test, y_pred, average='macro'))\n"
      ],
      "metadata": {
        "id": "FYtEvD3nLCbQ"
      },
      "id": "FYtEvD3nLCbQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#5. Imbalanced Data with Class Weights\n",
        "\n",
        "model = LogisticRegression(class_weight='balanced')\n",
        "model.fit(X_train, y_train)\n",
        "print(\"Imbalanced Accuracy:\", model.score(X_test, y_test))"
      ],
      "metadata": {
        "id": "yylleR4BLJwb"
      },
      "id": "yylleR4BLJwb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#6. Titanic Dataset - Missing Values Handling\n",
        "\n",
        "import pandas as pd\n",
        "df = pd.read_csv('titanic.csv')\n",
        "df['Age'].fillna(df['Age'].median(), inplace=True)\n",
        "df['Embarked'].fillna(df['Embarked'].mode()[0], inplace=True)"
      ],
      "metadata": {
        "id": "syOvJRTWLOUu"
      },
      "id": "syOvJRTWLOUu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#7. Feature Scaling (Standardization)\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "gm3IKAbALb-K"
      },
      "id": "gm3IKAbALb-K",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#8. ROC-AUC Score\n",
        "\n",
        "from sklearn.metrics import roc_auc_score\n",
        "prob = model.predict_proba(X_test)\n",
        "print(\"ROC-AUC:\", roc_auc_score(y_test, prob, multi_class='ovr'))\n"
      ],
      "metadata": {
        "id": "zPruBvfDLgLB"
      },
      "id": "zPruBvfDLgLB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#9. Custom Learning Rate (C=0.5)\n",
        "\n",
        "model = LogisticRegression(C=0.5)\n",
        "model.fit(X_train, y_train)\n",
        "print(\"Accuracy:\", model.score(X_test, y_test))\n"
      ],
      "metadata": {
        "id": "EcdghehRLqVC"
      },
      "id": "EcdghehRLqVC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#10. Feature Importance\n",
        "\n",
        "import numpy as np\n",
        "print(\"Feature Importance:\", np.abs(model.coef_))\n"
      ],
      "metadata": {
        "id": "tY2QZzY7LuNQ"
      },
      "id": "tY2QZzY7LuNQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#11. Cohen‚Äôs Kappa Score\n",
        "\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "print(\"Cohen Kappa Score:\", cohen_kappa_score(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "qY5f1x55Lxmk"
      },
      "id": "qY5f1x55Lxmk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#12. Precision-Recall Curve\n",
        "\n",
        "from sklearn.metrics import PrecisionRecallDisplay\n",
        "PrecisionRecallDisplay.from_estimator(model, X_test, y_test)\n",
        "\n"
      ],
      "metadata": {
        "id": "w9dZ9DUEL1DQ"
      },
      "id": "w9dZ9DUEL1DQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#13. Solvers Comparison\n",
        "\n",
        "for solver in ['liblinear', 'saga', 'lbfgs']:\n",
        "    m = LogisticRegression(solver=solver, max_iter=200)\n",
        "    m.fit(X_train, y_train)\n",
        "    print(solver, \"accuracy:\", m.score(X_test, y_test))\n"
      ],
      "metadata": {
        "id": "v3KMqw-SL5X6"
      },
      "id": "v3KMqw-SL5X6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#14. Matthews Correlation Coefficient (MCC)\n",
        "\n",
        "from sklearn.metrics import matthews_corrcoef\n",
        "print(\"MCC:\", matthews_corrcoef(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "lrlNUBgTL82Z"
      },
      "id": "lrlNUBgTL82Z",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#15. Compare Raw vs Standardized Accuracy\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "X_train_s, X_test_s, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3)\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train_s, y_train)\n",
        "print(\"Scaled Accuracy:\", model.score(X_test_s, y_test))\n"
      ],
      "metadata": {
        "id": "EIsCbHtyMCOZ"
      },
      "id": "EIsCbHtyMCOZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#16. Optimal C using Cross-Validation\n",
        "\n",
        "from sklearn.model_selection import cross_val_score\n",
        "for c in [0.01, 0.1, 1, 10]:\n",
        "    score = cross_val_score(LogisticRegression(C=c), X, y, cv=5).mean()\n",
        "    print(\"C=\", c, \"Score:\", score)\n"
      ],
      "metadata": {
        "id": "j5QxCLn5MInR"
      },
      "id": "j5QxCLn5MInR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#17. Save and Load Model with joblib\n",
        "\n",
        "import joblib\n",
        "joblib.dump(model, 'logreg_model.pkl')\n",
        "model_loaded = joblib.load('logreg_model.pkl')\n",
        "print(\"Loaded model accuracy:\", model_loaded.score(X_test, y_test))\n"
      ],
      "metadata": {
        "id": "QJtjJgQYMVBT"
      },
      "id": "QJtjJgQYMVBT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#18. Train Logistic Regression on the Titanic dataset, handle missing values, and evaluate performance\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Titanic dataset\n",
        "df = pd.read_csv(\"https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv\")\n",
        "\n",
        "# Basic preprocessing\n",
        "df = df[['Survived', 'Pclass', 'Sex', 'Age', 'Fare']].dropna()\n",
        "df['Sex'] = df['Sex'].map({'male': 0, 'female': 1})\n",
        "\n",
        "X = df.drop('Survived', axis=1)\n",
        "y = df['Survived']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)\n",
        "print(\"Accuracy:\", accuracy_score(y_test, model.predict(X_test)))\n"
      ],
      "metadata": {
        "id": "_zlCSYfQMZr1"
      },
      "id": "_zlCSYfQMZr1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#19. Apply feature scaling (Standardization) before training a Logistic Regression model\n",
        "\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3)\n",
        "\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)\n",
        "print(\"Accuracy with scaling:\", model.score(X_test, y_test))\n"
      ],
      "metadata": {
        "id": "P9LxR3RTMnWM"
      },
      "id": "P9LxR3RTMnWM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#20. Evaluate performance using ROC-AUC score\n",
        "\n",
        "from sklearn.metrics import roc_auc_score\n",
        "probs = model.predict_proba(X_test)[:, 1]\n",
        "print(\"ROC-AUC Score:\", roc_auc_score(y_test, probs))\n"
      ],
      "metadata": {
        "id": "sWS9a4UxMr_e"
      },
      "id": "sWS9a4UxMr_e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#21. Train with custom learning rate (C = 0.5) and evaluate\n",
        "\n",
        "model = LogisticRegression(C=0.5)\n",
        "model.fit(X_train, y_train)\n",
        "print(\"Accuracy:\", model.score(X_test, y_test))\n",
        "\n"
      ],
      "metadata": {
        "id": "UJOMK0lTMv9o"
      },
      "id": "UJOMK0lTMv9o",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#22. Identify important features based on model coefficients\n",
        "\n",
        "\n",
        "importance = model.coef_[0]\n",
        "for i, v in enumerate(importance):\n",
        "    print(f\"Feature {i}: {v:.4f}\")\n"
      ],
      "metadata": {
        "id": "9vVslejCMz71"
      },
      "id": "9vVslejCMz71",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#23. Evaluate performance using Cohen‚Äôs Kappa Score\n",
        "\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "print(\"Cohen Kappa Score:\", cohen_kappa_score(y_test, model.predict(X_test)))"
      ],
      "metadata": {
        "id": "aICbcWdrM4Dw"
      },
      "id": "aICbcWdrM4Dw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#24. Visualize the Precision-Recall Curve for binary classification\n",
        "\n",
        "from sklearn.metrics import PrecisionRecallDisplay\n",
        "PrecisionRecallDisplay.from_estimator(model, X_test, y_test)\n"
      ],
      "metadata": {
        "id": "eUvvKcRvM8SM"
      },
      "id": "eUvvKcRvM8SM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#25. Compare accuracy of different solvers: liblinear, saga, lbfgs\n",
        "\n",
        "\n",
        "solvers = ['liblinear', 'saga', 'lbfgs']\n",
        "for solver in solvers:\n",
        "    model = LogisticRegression(solver=solver, max_iter=200)\n",
        "    model.fit(X_train, y_train)\n",
        "    print(f\"{solver} accuracy:\", model.score(X_test, y_test))\n"
      ],
      "metadata": {
        "id": "ne4V2DEiNAJC"
      },
      "id": "ne4V2DEiNAJC",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}