{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "21ba7fd8",
      "metadata": {
        "id": "21ba7fd8"
      },
      "source": [
        "### Can we use Bagging for regression problems"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "12ea4f66",
      "metadata": {
        "id": "12ea4f66"
      },
      "outputs": [],
      "source": [
        "# Answer\n",
        "\"\"\"\n",
        "Yes, Bagging can be used for regression problems. The Bagging Regressor combines the predictions from multiple base regressors (like Decision Trees) trained on different subsets of the dataset, and averages their outputs for a final prediction.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ef6c9fc6",
      "metadata": {
        "id": "ef6c9fc6"
      },
      "source": [
        "### What is the difference between multiple model training and single model training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b81c6472",
      "metadata": {
        "id": "b81c6472"
      },
      "outputs": [],
      "source": [
        "# Answer\n",
        "\"\"\"\n",
        "Single model training involves using one algorithm on the training data to make predictions. Multiple model training (ensemble learning) uses several models and combines their predictions to improve overall performance and robustness.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bdc5a299",
      "metadata": {
        "id": "bdc5a299"
      },
      "source": [
        "### Explain the concept of feature randomness in Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b3ab3320",
      "metadata": {
        "id": "b3ab3320"
      },
      "outputs": [],
      "source": [
        "# Answer\n",
        "\"\"\"\n",
        "In Random Forest, feature randomness is introduced by selecting a random subset of features at each split point in a decision tree. This reduces correlation between trees and improves model generalization.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4f2fb737",
      "metadata": {
        "id": "4f2fb737"
      },
      "source": [
        "### What is OOB (Out-of-Bag) Score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2de9f89e",
      "metadata": {
        "id": "2de9f89e"
      },
      "outputs": [],
      "source": [
        "# Answer\n",
        "\"\"\"\n",
        "OOB Score is an internal validation method for ensemble models like Random Forests. It measures prediction accuracy using the samples not included in the bootstrap sample for each tree.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0798a80d",
      "metadata": {
        "id": "0798a80d"
      },
      "source": [
        "### How can you measure the importance of features in a Random Forest model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "87afb939",
      "metadata": {
        "id": "87afb939"
      },
      "outputs": [],
      "source": [
        "# Answer\n",
        "\"\"\"\n",
        "Feature importance in Random Forest can be measured by evaluating how much each feature decreases the impurity (like Gini or Entropy) or by computing permutation importance on the model performance.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b097c5ce",
      "metadata": {
        "id": "b097c5ce"
      },
      "source": [
        "### Explain the working principle of a Bagging Classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f47e9edd",
      "metadata": {
        "id": "f47e9edd"
      },
      "outputs": [],
      "source": [
        "# Answer\n",
        "\"\"\"\n",
        "A Bagging Classifier builds multiple instances of a base estimator on different bootstrap samples of the training dataset and combines their predictions through majority voting (for classification).\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2d482563",
      "metadata": {
        "id": "2d482563"
      },
      "source": [
        "### How do you evaluate a Bagging Classifierâ€™s performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "77c9a840",
      "metadata": {
        "id": "77c9a840"
      },
      "outputs": [],
      "source": [
        "# Answer\n",
        "\"\"\"\n",
        "You can evaluate it using classification metrics such as accuracy, precision, recall, F1-score, and confusion matrix on a validation or test dataset.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7915f0c0",
      "metadata": {
        "id": "7915f0c0"
      },
      "source": [
        "### How does a Bagging Regressor work"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "308a713c",
      "metadata": {
        "id": "308a713c"
      },
      "outputs": [],
      "source": [
        "# Answer\n",
        "\"\"\"\n",
        "A Bagging Regressor trains multiple regressors on random subsets of the data and averages their predictions to produce a final output, reducing variance and improving robustness.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1087335d",
      "metadata": {
        "id": "1087335d"
      },
      "source": [
        "### What is the main advantage of ensemble techniques"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "da1e7f6f",
      "metadata": {
        "id": "da1e7f6f"
      },
      "outputs": [],
      "source": [
        "# Answer\n",
        "\"\"\"\n",
        "The main advantage is improved predictive performance due to the aggregation of multiple models, which reduces variance, bias, or both.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "10390336",
      "metadata": {
        "id": "10390336"
      },
      "source": [
        "### What is the main challenge of ensemble methods"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0f7001cd",
      "metadata": {
        "id": "0f7001cd"
      },
      "outputs": [],
      "source": [
        "# Answer\n",
        "\"\"\"\n",
        "The main challenge includes increased computational cost and complexity in model interpretation and maintenance.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3418ee63",
      "metadata": {
        "id": "3418ee63"
      },
      "source": [
        "### Explain the key idea behind ensemble techniques"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "70615631",
      "metadata": {
        "id": "70615631"
      },
      "outputs": [],
      "source": [
        "# Answer\n",
        "\"\"\"\n",
        "The key idea is to combine predictions from multiple models to achieve better performance than any individual model.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "44cd1f10",
      "metadata": {
        "id": "44cd1f10"
      },
      "source": [
        "### What is a Random Forest Classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3e280c43",
      "metadata": {
        "id": "3e280c43"
      },
      "outputs": [],
      "source": [
        "# Answer\n",
        "\"\"\"\n",
        "A Random Forest Classifier is an ensemble learning method that uses a collection of decision trees, trained on different subsets of data and features, and predicts the class by majority vote.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9798cf87",
      "metadata": {
        "id": "9798cf87"
      },
      "source": [
        "### What are the main types of ensemble techniques"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "76e9129d",
      "metadata": {
        "id": "76e9129d"
      },
      "outputs": [],
      "source": [
        "# Answer\n",
        "\"\"\"\n",
        "The main types are Bagging, Boosting, and Stacking.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4674cd5a",
      "metadata": {
        "id": "4674cd5a"
      },
      "source": [
        "### What is ensemble learning in machine learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a2f043a7",
      "metadata": {
        "id": "a2f043a7"
      },
      "outputs": [],
      "source": [
        "# Answer\n",
        "\"\"\"\n",
        "Ensemble learning combines multiple models to produce a more accurate and robust prediction than a single model.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "521a2404",
      "metadata": {
        "id": "521a2404"
      },
      "source": [
        "### When should we avoid using ensemble methods"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8ee06309",
      "metadata": {
        "id": "8ee06309"
      },
      "outputs": [],
      "source": [
        "# Answer\n",
        "\"\"\"\n",
        "Avoid when interpretability is crucial, or the computational resources are limited, or the base model already performs optimally.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2c43aabd",
      "metadata": {
        "id": "2c43aabd"
      },
      "source": [
        "### How does Bagging help in reducing overfitting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b1018b53",
      "metadata": {
        "id": "b1018b53"
      },
      "outputs": [],
      "source": [
        "# Answer\n",
        "\"\"\"\n",
        "Bagging reduces overfitting by training models on different subsets of the data, thus reducing variance.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e36d6d22",
      "metadata": {
        "id": "e36d6d22"
      },
      "source": [
        "### Why is Random Forest better than a single Decision Tree"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "524a45eb",
      "metadata": {
        "id": "524a45eb"
      },
      "outputs": [],
      "source": [
        "# Answer\n",
        "\"\"\"\n",
        "Random Forest reduces overfitting and improves generalization by averaging predictions from multiple de-correlated trees.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4ea7865d",
      "metadata": {
        "id": "4ea7865d"
      },
      "source": [
        "### What is the role of bootstrap sampling in Bagging"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8ebfe878",
      "metadata": {
        "id": "8ebfe878"
      },
      "outputs": [],
      "source": [
        "# Answer\n",
        "\"\"\"\n",
        "Bootstrap sampling creates diverse training sets for each base learner, introducing variability and reducing overfitting.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dbcf76de",
      "metadata": {
        "id": "dbcf76de"
      },
      "source": [
        "### What are some real-world applications of ensemble techniques"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "208ae2a1",
      "metadata": {
        "id": "208ae2a1"
      },
      "outputs": [],
      "source": [
        "# Answer\n",
        "\"\"\"\n",
        "Applications include fraud detection, spam filtering, credit scoring, medical diagnosis, and stock market prediction.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6f5f22a7",
      "metadata": {
        "id": "6f5f22a7"
      },
      "source": [
        "### What is the difference between Bagging and Boosting?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fbad674c",
      "metadata": {
        "id": "fbad674c"
      },
      "outputs": [],
      "source": [
        "# Answer\n",
        "\"\"\"\n",
        "Bagging builds independent models in parallel to reduce variance, while Boosting builds models sequentially to reduce bias and improve prediction accuracy.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "12a694f2",
      "metadata": {
        "id": "12a694f2"
      },
      "source": [
        "### Train a Bagging Classifier using Decision Trees on a sample dataset and print model accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cb91a263",
      "metadata": {
        "id": "cb91a263"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "model = BaggingClassifier(base_estimator=DecisionTreeClassifier(), n_estimators=10, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8760f832",
      "metadata": {
        "id": "8760f832"
      },
      "source": [
        "### Train a Bagging Regressor using Decision Trees and evaluate using Mean Squared Error (MSE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cdd10246",
      "metadata": {
        "id": "cdd10246"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import BaggingRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "X, y = make_regression(n_samples=1000, n_features=20, noise=0.1, random_state=42)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "regressor = BaggingRegressor(base_estimator=DecisionTreeRegressor(), n_estimators=10, random_state=42)\n",
        "regressor.fit(X_train, y_train)\n",
        "y_pred = regressor.predict(X_test)\n",
        "\n",
        "print(\"Mean Squared Error:\", mean_squared_error(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6f368758",
      "metadata": {
        "id": "6f368758"
      },
      "source": [
        "### Train a Random Forest Classifier on the Breast Cancer dataset and print feature importance scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eeda7110",
      "metadata": {
        "id": "eeda7110"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "importances = clf.feature_importances_\n",
        "for name, importance in zip(data.feature_names, importances):\n",
        "    print(f\"{name}: {importance:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7773a426",
      "metadata": {
        "id": "7773a426"
      },
      "source": [
        "### Train a Random Forest Regressor and compare its performance with a single Decision Tree"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5dfee5e7",
      "metadata": {
        "id": "5dfee5e7"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# Decision Tree\n",
        "tree = DecisionTreeRegressor(random_state=42)\n",
        "tree.fit(X_train, y_train)\n",
        "tree_pred = tree.predict(X_test)\n",
        "\n",
        "# Random Forest\n",
        "forest = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "forest.fit(X_train, y_train)\n",
        "forest_pred = forest.predict(X_test)\n",
        "\n",
        "print(\"Decision Tree R2 Score:\", r2_score(y_test, tree_pred))\n",
        "print(\"Random Forest R2 Score:\", r2_score(y_test, forest_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "105def3f",
      "metadata": {
        "id": "105def3f"
      },
      "source": [
        "### Compute the Out-of-Bag (OOB) Score for a Random Forest Classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a35b7072",
      "metadata": {
        "id": "a35b7072"
      },
      "outputs": [],
      "source": [
        "oob_clf = RandomForestClassifier(n_estimators=100, oob_score=True, random_state=42)\n",
        "oob_clf.fit(X_train, y_train)\n",
        "print(\"OOB Score:\", oob_clf.oob_score_)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}