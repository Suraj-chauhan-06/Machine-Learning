{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1. What is a Support Vector Machine (SVM)?\n",
        "\n",
        "SVM is a supervised learning algorithm that finds the optimal hyperplane to separate data into different classes.\n",
        "\n",
        "\n",
        "2. Difference between Hard Margin and Soft Margin SVM?\n",
        "\n",
        "\n",
        "Hard Margin: No misclassifications allowed (strict separation).\n",
        "\n",
        "Soft Margin: Allows some errors to improve generalization.\n",
        "\n",
        "\n",
        "3. Mathematical intuition behind SVM?\n",
        "\n",
        "SVM aims to maximize the margin between classes by solving a convex optimization problem.\n",
        "\n",
        "\n",
        "4. Role of Lagrange Multipliers in SVM?\n",
        "\n",
        "They convert the constrained optimization problem into a solvable dual form.\n",
        "\n",
        "\n",
        "5. What are Support Vectors in SVM?\n",
        "\n",
        "These are data points closest to the hyperplane; they define the margin.\n",
        "\n",
        "\n",
        "6. What is a Support Vector Classifier (SVC)?\n",
        "\n",
        "SVC is SVM used for classification problems.\n",
        "\n",
        "\n",
        "\n",
        "7. What is a Support Vector Regressor (SVR)?\n",
        "\n",
        "SVR is an SVM variant used for regression tasks, fitting data within a tolerance margin.\n",
        "\n",
        "\n",
        "8. What is the Kernel Trick in SVM?\n",
        "\n",
        "A technique to handle non-linear data by mapping it into higher dimensions using kernels.\n",
        "\n",
        "\n",
        "9. Compare Linear, Polynomial, and RBF Kernels.\n",
        "\n",
        "Linear: Straight-line separation\n",
        "\n",
        "Polynomial: Captures curved relationships\n",
        "\n",
        "RBF: Handles complex, non-linear patterns well\n",
        "\n",
        "\n",
        "10. Effect of the C parameter in SVM?\n",
        "\n",
        "C controls trade-off between margin width and classification error; small C = wider margin, large C = less tolerance for errors.\n",
        "\n",
        "\n",
        "\n",
        "11. Role of Gamma in RBF Kernel SVM?\n",
        "\n",
        "Gamma defines how far the influence of a training example reaches. High gamma = close influence.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "12. What is the Na√Øve Bayes classifier, and why is it called \"Na√Øve\"?\n",
        "\n",
        "It applies Bayes‚Äô Theorem assuming all features are independent (hence ‚Äúna√Øve‚Äù).\n",
        "\n",
        "\n",
        "13. What is Bayes‚Äô Theorem?\n",
        "ùëÉ\n",
        "(\n",
        "ùê¥\n",
        "‚à£\n",
        "ùêµ\n",
        ")\n",
        "=\n",
        "ùëÉ\n",
        "(\n",
        "ùêµ\n",
        "‚à£\n",
        "ùê¥\n",
        ")\n",
        "‚ãÖ\n",
        "ùëÉ\n",
        "(\n",
        "ùê¥\n",
        ")\n",
        "ùëÉ\n",
        "(\n",
        "ùêµ\n",
        ")\n",
        "P(A‚à£B)=\n",
        "P(B)\n",
        "P(B‚à£A)‚ãÖP(A)\n",
        "‚Äã\n",
        "\n",
        "\n",
        "\n",
        "14. Gaussian, Multinomial, and Bernoulli Na√Øve Bayes:\n",
        "\n",
        "Gaussian: For continuous, normally-distributed features\n",
        "\n",
        "Multinomial: For count-based text data\n",
        "\n",
        "Bernoulli: For binary/boolean features\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "15. When should you use Gaussian Na√Øve Bayes?\n",
        "\n",
        "When features are continuous and follow a normal distribution.\n",
        "\n",
        "\n",
        "\n",
        "16. Key assumptions made by Na√Øve Bayes:\n",
        "\n",
        "Features are conditionally independent\n",
        "\n",
        "All features contribute equally\n",
        "\n",
        "\n",
        "\n",
        "17. Advantages and disadvantages of Na√Øve Bayes:\n",
        "\n",
        "Fast, simple, good for high-dimensional data\n",
        "‚àí Assumes independence, less accurate than complex models\n",
        "\n",
        "\n",
        "\n",
        "18. Why is Na√Øve Bayes good for text classification?\n",
        "\n",
        "It's efficient with large vocabularies and works well with sparse data.\n",
        "\n",
        "\n",
        "\n",
        "19. Compare SVM and Na√Øve Bayes for classification:\n",
        "\n",
        "SVM: More accurate, handles complex data better\n",
        "\n",
        "NB: Simpler, faster, good for text and noisy data\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "20. How does Laplace Smoothing help in Na√Øve Bayes?\n",
        "\n",
        "It avoids zero probabilities by adding 1 to all feature counts."
      ],
      "metadata": {
        "id": "B3qEukbGRwrJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#21: SVM Classifier on Iris Dataset\n",
        "\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train SVM\n",
        "svm = SVC(kernel='linear')\n",
        "svm.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred = svm.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy:.2f}\")"
      ],
      "metadata": {
        "id": "jRfnU42DSebA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#22: SVM with Linear and RBF Kernels on Wine Dataset\n",
        "\n",
        "\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load dataset\n",
        "wine = datasets.load_wine()\n",
        "X = wine.data\n",
        "y = wine.target\n",
        "\n",
        "# Scale data\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train linear SVM\n",
        "linear_svm = SVC(kernel='linear')\n",
        "linear_svm.fit(X_train, y_train)\n",
        "linear_pred = linear_svm.predict(X_test)\n",
        "linear_acc = accuracy_score(y_test, linear_pred)\n",
        "\n",
        "# Train RBF SVM\n",
        "rbf_svm = SVC(kernel='rbf')\n",
        "rbf_svm.fit(X_train, y_train)\n",
        "rbf_pred = rbf_svm.predict(X_test)\n",
        "rbf_acc = accuracy_score(y_test, rbf_pred)\n",
        "\n",
        "print(f\"Linear Kernel Accuracy: {linear_acc:.2f}\")\n",
        "print(f\"RBF Kernel Accuracy: {rbf_acc:.2f}\")"
      ],
      "metadata": {
        "id": "EdMdkEecUKoY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#23: SVM Regressor on Housing Dataset\n",
        "\n",
        "\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load dataset\n",
        "housing = fetch_california_housing()\n",
        "X = housing.data\n",
        "y = housing.target\n",
        "\n",
        "# Scale data\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train SVR\n",
        "svr = SVR(kernel='rbf')\n",
        "svr.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred = svr.predict(X_test)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(f\"Mean Squared Error: {mse:.2f}\")"
      ],
      "metadata": {
        "id": "K0CFJlyfUPMG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#24: SVM with Polynomial Kernel and Decision Boundary Visualization\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import datasets\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.inspection import DecisionBoundaryDisplay\n",
        "\n",
        "# Load dataset (using only 2 features for visualization)\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data[:, :2]  # Only first two features\n",
        "y = iris.target\n",
        "\n",
        "# Train SVM with polynomial kernel\n",
        "svm = SVC(kernel='poly', degree=3)\n",
        "svm.fit(X, y)\n",
        "\n",
        "# Create decision boundary plot\n",
        "DecisionBoundaryDisplay.from_estimator(\n",
        "    svm,\n",
        "    X,\n",
        "    cmap=plt.cm.Paired,\n",
        "    response_method=\"predict\"\n",
        ")\n",
        "\n",
        "# Plot training points\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y, edgecolors='k')\n",
        "plt.xlabel('Sepal length')\n",
        "plt.ylabel('Sepal width')\n",
        "plt.title('SVM with Polynomial Kernel Decision Boundary')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "IokurIllUThR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#25: Gaussian Naive Bayes on Breast Cancer Dataset\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train Gaussian Naive Bayes\n",
        "gnb = GaussianNB()\n",
        "gnb.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred = gnb.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy:.2f}\")"
      ],
      "metadata": {
        "id": "XEPmH3xHUay1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#26: Multinomial Naive Bayes for Text Classification\n",
        "\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "categories = ['alt.atheism', 'soc.religion.christian', 'comp.graphics', 'sci.med']\n",
        "train = fetch_20newsgroups(subset='train', categories=categories)\n",
        "test = fetch_20newsgroups(subset='test', categories=categories)\n",
        "\n",
        "# Create pipeline with TF-IDF and Multinomial NB\n",
        "model = make_pipeline(TfidfVectorizer(), MultinomialNB())\n",
        "model.fit(train.data, train.target)\n",
        "\n",
        "# Predict and evaluate\n",
        "predicted = model.predict(test.data)\n",
        "accuracy = accuracy_score(test.target, predicted)\n",
        "print(f\"Accuracy: {accuracy:.2f}\")"
      ],
      "metadata": {
        "id": "1qoeYlBXUhyw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#27: SVM with Different C Values and Decision Boundaries\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import datasets\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.inspection import DecisionBoundaryDisplay\n",
        "\n",
        "# Load dataset (using only 2 features for visualization)\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data[:, :2]  # Only first two features\n",
        "y = iris.target\n",
        "\n",
        "# Different C values to try\n",
        "C_values = [0.1, 1, 10, 100]\n",
        "\n",
        "plt.figure(figsize=(15, 10))\n",
        "for i, C in enumerate(C_values, 1):\n",
        "    # Train SVM\n",
        "    svm = SVC(kernel='linear', C=C)\n",
        "    svm.fit(X, y)\n",
        "\n",
        "    # Create subplot\n",
        "    plt.subplot(2, 2, i)\n",
        "    DecisionBoundaryDisplay.from_estimator(\n",
        "        svm,\n",
        "        X,\n",
        "        cmap=plt.cm.Paired,\n",
        "        response_method=\"predict\"\n",
        "    )\n",
        "    plt.scatter(X[:, 0], X[:, 1], c=y, edgecolors='k')\n",
        "    plt.title(f'SVM Decision Boundary (C={C})')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Dj5ZdtXgUm3y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#28: Bernoulli Naive Bayes for Binary Classification\n",
        "\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import BernoulliNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Create binary dataset with binary features\n",
        "X, y = make_classification(n_samples=1000, n_features=20, n_informative=15,\n",
        "                          n_redundant=2, n_classes=2, random_state=42)\n",
        "X = (X > 0).astype(int)  # Convert to binary features\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train Bernoulli Naive Bayes\n",
        "bnb = BernoulliNB()\n",
        "bnb.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred = bnb.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy:.2f}\")"
      ],
      "metadata": {
        "id": "YzUMyhVoUsd0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#29: Feature Scaling for SVM\n",
        "\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "wine = datasets.load_wine()\n",
        "X = wine.data\n",
        "y = wine.target\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Without scaling\n",
        "svm_unscaled = SVC(kernel='rbf')\n",
        "svm_unscaled.fit(X_train, y_train)\n",
        "y_pred_unscaled = svm_unscaled.predict(X_test)\n",
        "acc_unscaled = accuracy_score(y_test, y_pred_unscaled)\n",
        "\n",
        "# With scaling\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "svm_scaled = SVC(kernel='rbf')\n",
        "svm_scaled.fit(X_train_scaled, y_train)\n",
        "y_pred_scaled = svm_scaled.predict(X_test_scaled)\n",
        "acc_scaled = accuracy_score(y_test, y_pred_scaled)\n",
        "\n",
        "print(f\"Accuracy without scaling: {acc_unscaled:.2f}\")\n",
        "print(f\"Accuracy with scaling: {acc_scaled:.2f}\")"
      ],
      "metadata": {
        "id": "W9ni1mG0UxVb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#30: Gaussian Naive Bayes with Laplace Smoothing\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Without Laplace smoothing (var_smoothing=0)\n",
        "gnb_no_smooth = GaussianNB(var_smoothing=0)\n",
        "gnb_no_smooth.fit(X_train, y_train)\n",
        "y_pred_no_smooth = gnb_no_smooth.predict(X_test)\n",
        "acc_no_smooth = accuracy_score(y_test, y_pred_no_smooth)\n",
        "\n",
        "# With Laplace smoothing (default var_smoothing=1e-9)\n",
        "gnb_smooth = GaussianNB()\n",
        "gnb_smooth.fit(X_train, y_train)\n",
        "y_pred_smooth = gnb_smooth.predict(X_test)\n",
        "acc_smooth = accuracy_score(y_test, y_pred_smooth)\n",
        "\n",
        "print(f\"Accuracy without smoothing: {acc_no_smooth:.2f}\")\n",
        "print(f\"Accuracy with smoothing: {acc_smooth:.2f}\")"
      ],
      "metadata": {
        "id": "6nfbzAIaVAFa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#31: SVM with GridSearchCV\n",
        "\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Define parameter grid\n",
        "param_grid = {\n",
        "    'C': [0.1, 1, 10, 100],\n",
        "    'gamma': [1, 0.1, 0.01, 0.001],\n",
        "    'kernel': ['rbf', 'linear', 'poly']\n",
        "}\n",
        "\n",
        "# Grid search\n",
        "grid = GridSearchCV(SVC(), param_grid, refit=True, verbose=2)\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "# Best parameters\n",
        "print(f\"Best parameters: {grid.best_params_}\")\n",
        "\n",
        "# Predict with best model\n",
        "y_pred = grid.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy:.2f}\")"
      ],
      "metadata": {
        "id": "g8r8cNYjVGMJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#32: SVM on Imbalanced Dataset with Class Weighting\n",
        "\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Create imbalanced dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=20, n_informative=15,\n",
        "                          n_redundant=2, weights=[0.9, 0.1], random_state=42)\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Without class weighting\n",
        "svm_noweight = SVC(kernel='linear')\n",
        "svm_noweight.fit(X_train, y_train)\n",
        "y_pred_noweight = svm_noweight.predict(X_test)\n",
        "acc_noweight = accuracy_score(y_test, y_pred_noweight)\n",
        "print(\"Without class weighting:\")\n",
        "print(f\"Accuracy: {acc_noweight:.2f}\")\n",
        "print(classification_report(y_test, y_pred_noweight))\n",
        "\n",
        "# With class weighting\n",
        "svm_weighted = SVC(kernel='linear', class_weight='balanced')\n",
        "svm_weighted.fit(X_train, y_train)\n",
        "y_pred_weighted = svm_weighted.predict(X_test)\n",
        "acc_weighted = accuracy_score(y_test, y_pred_weighted)\n",
        "print(\"\\nWith class weighting:\")\n",
        "print(f\"Accuracy: {acc_weighted:.2f}\")\n",
        "print(classification_report(y_test, y_pred_weighted))"
      ],
      "metadata": {
        "id": "NHWawbM6VL6e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#33: Naive Bayes for Spam Detection\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Example email dataset (you would replace this with your actual dataset)\n",
        "data = {\n",
        "    'text': [\n",
        "        'win money now', 'free lottery', 'meeting tomorrow',\n",
        "        'project update', 'buy cheap viagra', 'urgent help needed',\n",
        "        'congratulations you won', 'account verification required',\n",
        "        'team lunch next week', 'security alert'\n",
        "    ],\n",
        "    'label': [1, 1, 0, 0, 1, 0, 1, 1, 0, 0]  # 1=spam, 0=ham\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Vectorize text\n",
        "vectorizer = CountVectorizer()\n",
        "X = vectorizer.fit_transform(df['text'])\n",
        "y = df['label']\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train Naive Bayes\n",
        "nb = MultinomialNB()\n",
        "nb.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred = nb.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "id": "gXB1NmSDVSvT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#34: Compare SVM and Naive Bayes on Same Dataset\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Scale data (important for SVM)\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train SVM\n",
        "svm = SVC(kernel='rbf')\n",
        "svm.fit(X_train, y_train)\n",
        "y_pred_svm = svm.predict(X_test)\n",
        "acc_svm = accuracy_score(y_test, y_pred_svm)\n",
        "\n",
        "# Train Naive Bayes\n",
        "nb = GaussianNB()\n",
        "nb.fit(X_train, y_train)\n",
        "y_pred_nb = nb.predict(X_test)\n",
        "acc_nb = accuracy_score(y_test, y_pred_nb)\n",
        "\n",
        "print(f\"SVM Accuracy: {acc_svm:.2f}\")\n",
        "print(f\"Naive Bayes Accuracy: {acc_nb:.2f}\")"
      ],
      "metadata": {
        "id": "lwUQWB7lVX1w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#35: Feature Selection for Naive Bayes\n",
        "\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.feature_selection import SelectKBest, f_classif\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Without feature selection\n",
        "nb_all = GaussianNB()\n",
        "nb_all.fit(X_train, y_train)\n",
        "y_pred_all = nb_all.predict(X_test)\n",
        "acc_all = accuracy_score(y_test, y_pred_all)\n",
        "\n",
        "# With feature selection (select top 10 features)\n",
        "selector = SelectKBest(f_classif, k=10)\n",
        "X_train_selected = selector.fit_transform(X_train, y_train)\n",
        "X_test_selected = selector.transform(X_test)\n",
        "\n",
        "nb_selected = GaussianNB()\n",
        "nb_selected.fit(X_train_selected, y_train)\n",
        "y_pred_selected = nb_selected.predict(X_test_selected)\n",
        "acc_selected = accuracy_score(y_test, y_pred_selected)\n",
        "\n",
        "print(f\"Accuracy with all features: {acc_all:.2f}\")\n",
        "print(f\"Accuracy with selected features: {acc_selected:.2f}\")"
      ],
      "metadata": {
        "id": "ORqJ5vwIVdNv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#36: SVM with OvR and OvO on Wine Dataset\n",
        "\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.multiclass import OneVsRestClassifier, OneVsOneClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load dataset\n",
        "wine = datasets.load_wine()\n",
        "X = wine.data\n",
        "y = wine.target\n",
        "\n",
        "# Scale data\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# One-vs-Rest\n",
        "ovr = OneVsRestClassifier(SVC(kernel='linear'))\n",
        "ovr.fit(X_train, y_train)\n",
        "y_pred_ovr = ovr.predict(X_test)\n",
        "acc_ovr = accuracy_score(y_test, y_pred_ovr)\n",
        "\n",
        "# One-vs-One\n",
        "ovo = OneVsOneClassifier(SVC(kernel='linear'))\n",
        "ovo.fit(X_train, y_train)\n",
        "y_pred_ovo = ovo.predict(X_test)\n",
        "acc_ovo = accuracy_score(y_test, y_pred_ovo)\n",
        "\n",
        "print(f\"OvR Accuracy: {acc_ovr:.2f}\")\n",
        "print(f\"OvO Accuracy: {acc_ovo:.2f}\")"
      ],
      "metadata": {
        "id": "cKZIvsV2Vix8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#37: SVM with Different Kernels on Breast Cancer Dataset\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Scale data\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Different kernels to try\n",
        "kernels = ['linear', 'poly', 'rbf']\n",
        "\n",
        "for kernel in kernels:\n",
        "    svm = SVC(kernel=kernel)\n",
        "    svm.fit(X_train, y_train)\n",
        "    y_pred = svm.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    print(f\"Kernel: {kernel}, Accuracy: {accuracy:.2f}\")"
      ],
      "metadata": {
        "id": "QHOTCPldVonj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#38: SVM with Stratified K-Fold Cross-Validation\n",
        "\n",
        "\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load dataset\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Scale data\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "# Create SVM model\n",
        "svm = SVC(kernel='linear')\n",
        "\n",
        "# Stratified K-Fold CV\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "scores = cross_val_score(svm, X, y, cv=skf)\n",
        "\n",
        "# Print results\n",
        "print(\"Cross-validation scores:\", scores)\n",
        "print(f\"Average accuracy: {scores.mean():.2f} (+/- {scores.std() * 2:.2f})\")"
      ],
      "metadata": {
        "id": "3sVz2hteVvtN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#39: Naive Bayes with Different Priors\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Different priors to try\n",
        "priors = [\n",
        "    None,  # Let the model estimate\n",
        "    [0.5, 0.5],  # Equal priors\n",
        "    [0.3, 0.7],  # Skewed priors\n",
        "    [0.7, 0.3]   # Opposite skew\n",
        "]\n",
        "\n",
        "for prior in priors:\n",
        "    nb = GaussianNB(priors=prior)\n",
        "    nb.fit(X_train, y_train)\n",
        "    y_pred = nb.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    print(f\"Priors: {prior}, Accuracy: {accuracy:.2f}\")"
      ],
      "metadata": {
        "id": "SvJegwtQV1T0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#40: Recursive Feature Elimination for SVM\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Scale data\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Without RFE\n",
        "svm = SVC(kernel='linear')\n",
        "svm.fit(X_train, y_train)\n",
        "y_pred = svm.predict(X_test)\n",
        "acc = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy without RFE: {acc:.2f}\")\n",
        "\n",
        "# With RFE (select half the features)\n",
        "rfe = RFE(estimator=SVC(kernel='linear'), n_features_to_select=X.shape[1]//2)\n",
        "X_train_rfe = rfe.fit_transform(X_train, y_train)\n",
        "X_test_rfe = rfe.transform(X_test)\n",
        "\n",
        "svm_rfe = SVC(kernel='linear')\n",
        "svm_rfe.fit(X_train_rfe, y_train)\n",
        "y_pred_rfe = svm_rfe.predict(X_test_rfe)\n",
        "acc_rfe = accuracy_score(y_test, y_pred_rfe)\n",
        "print(f\"Accuracy with RFE: {acc_rfe:.2f}\")"
      ],
      "metadata": {
        "id": "3pux55kdV6dB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#41: SVM Evaluation with Precision, Recall, F1-Score\n",
        "\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "# Load dataset\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train SVM\n",
        "svm = SVC(kernel='linear')\n",
        "svm.fit(X_train, y_train)\n",
        "\n",
        "# Predict\n",
        "y_pred = svm.predict(X_test)\n",
        "\n",
        "# Evaluate\n",
        "precision = precision_score(y_test, y_pred, average='weighted')\n",
        "recall = recall_score(y_test, y_pred, average='weighted')\n",
        "f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "\n",
        "print(f\"Precision: {precision:.2f}\")\n",
        "print(f\"Recall: {recall:.2f}\")\n",
        "print(f\"F1-Score: {f1:.2f}\")"
      ],
      "metadata": {
        "id": "HcMkTLBPV_hd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#42: Naive Bayes Evaluation with Log Loss\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import log_loss\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train Naive Bayes\n",
        "nb = GaussianNB()\n",
        "nb.fit(X_train, y_train)\n",
        "\n",
        "# Get predicted probabilities\n",
        "y_proba = nb.predict_proba(X_test)\n",
        "\n",
        "# Calculate log loss\n",
        "loss = log_loss(y_test, y_proba)\n",
        "print(f\"Log Loss: {loss:.2f}\")"
      ],
      "metadata": {
        "id": "5NCwFfFGWFM_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#43: SVM Confusion Matrix Visualization\n",
        "\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load dataset\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train SVM\n",
        "svm = SVC(kernel='linear')\n",
        "svm.fit(X_train, y_train)\n",
        "\n",
        "# Predict\n",
        "y_pred = svm.predict(X_test)\n",
        "\n",
        "# Confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=iris.target_names,\n",
        "            yticklabels=iris.target_names)\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.title('Confusion Matrix for SVM Classifier')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "RnjbthxEWK-k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#44: SVR Evaluation with MAE\n",
        "\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load dataset\n",
        "housing = fetch_california_housing()\n",
        "X = housing.data\n",
        "y = housing.target\n",
        "\n",
        "# Scale data\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train SVR\n",
        "svr = SVR(kernel='rbf')\n",
        "svr.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred = svr.predict(X_test)\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "print(f\"Mean Absolute Error: {mae:.2f}\")"
      ],
      "metadata": {
        "id": "HzQm3k4nWQW9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#45: Naive Bayes ROC-AUC Evaluation\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train Naive Bayes\n",
        "nb = GaussianNB()\n",
        "nb.fit(X_train, y_train)\n",
        "\n",
        "# Get predicted probabilities\n",
        "y_proba = nb.predict_proba(X_test)[:, 1]  # Probability of positive class\n",
        "\n",
        "# Calculate ROC-AUC\n",
        "roc_auc = roc_auc_score(y_test, y_proba)\n",
        "print(f\"ROC-AUC Score: {roc_auc:.2f}\")"
      ],
      "metadata": {
        "id": "lSr_JrBKWVkV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#46: SVM Precision-Recall Curve Visualization\n",
        "\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load dataset (binary for precision-recall curve)\n",
        "data = datasets.load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train SVM\n",
        "svm = SVC(kernel='linear', probability=True)\n",
        "svm.fit(X_train, y_train)\n",
        "\n",
        "# Get predicted probabilities\n",
        "y_proba = svm.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Calculate precision-recall curve\n",
        "precision, recall, thresholds = precision_recall_curve(y_test, y_proba)\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(recall, precision, marker='.')\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.title('Precision-Recall Curve for SVM Classifier')\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "qelYDTdyWa5X"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}